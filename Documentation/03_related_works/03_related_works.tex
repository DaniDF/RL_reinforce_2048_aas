From the release of the game in 2014 a lot of researchers tried to win the game and achieve the 2048 tile on the board.
P. Rodgers and J. Levine\cite{ai-strat} 2014 compared different approach to learning how to win 2048 game and the best strategies. They used Montecarlo Tree Search (MCTS) to estimate the goodness of each bord configuration. They expected at the end of the simulation to have an asymmetric tree, that will result in an easiest situation for concentrating the efforts on the potentially good moves. Instead, they discovered that playing 2048 ends in symmetric trees.
They also implemented Averaged Depth-Limited Search (ADLS), which is an approximation of Expectimax that runs multiple simulations. This approach does not try to simulate all possibilities, but the more likely situations will appear more often in the simulations. They concluded that ADLS is a lot better than MCTS because ADLS uses an evaluating function that is fundamental in a game such as 2048, which has a lot of randomness and where minimizing risks is key.
Marcin Szubert and Wojciech Laskowski\cite{td-after} developed a TD(0)\cite{td} agent, TD-afterstate, that is capable of winning the 2048 game without incorporating human expertise. They showed that TD-afterstate can achieve a win rate of 98\% with a powerful evaluation function. They also highlighted that learning afterstates is a viable way to learn state values in a stochastic environment where it is difficult to obtain the following state.